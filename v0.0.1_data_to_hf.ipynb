{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00ec2ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mingjia-guan/workspaces/MABe-2025/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Created visualization directory: figures\n",
      "\n",
      "================================================================================\n",
      "               üê≠ MABe CHALLENGE - ENHANCED VISUALIZATION VERSION üê≠\n",
      "================================================================================\n",
      "üìÖ Start Time: 2025-10-02 11:39:01\n",
      "üîß Mode: SUBMIT\n",
      "üìä Visualizations: ENABLED\n",
      "üíæ Save Plots: YES\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "MABe Challenge - Enhanced with Comprehensive Visualizations (Fault-Tolerant)\n",
    "============================================================================\n",
    "Complete implementation with detailed analytics and robust error handling\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "import warnings\n",
    "import json\n",
    "import os\n",
    "import gc\n",
    "from collections import defaultdict, Counter\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# Import visualization libraries with error handling\n",
    "try:\n",
    "    import plotly.graph_objects as go\n",
    "    import plotly.express as px\n",
    "    from plotly.subplots import make_subplots\n",
    "    PLOTLY_AVAILABLE = True\n",
    "except ImportError:\n",
    "    PLOTLY_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è Plotly not available - interactive visualizations disabled\")\n",
    "\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.patches import Rectangle\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "from sklearn.base import ClassifierMixin, BaseEstimator, clone\n",
    "from sklearn.model_selection import cross_val_predict, GroupKFold\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import f1_score, confusion_matrix, precision_recall_curve\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import polars as pl\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Union, List, Optional, Dict, Iterator\n",
    "from datasets import Dataset, DatasetDict\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Enhanced visualization settings\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "plt.rcParams['font.size'] = 11\n",
    "plt.rcParams['axes.titlesize'] = 13\n",
    "plt.rcParams['axes.labelsize'] = 11\n",
    "plt.rcParams['xtick.labelsize'] = 10\n",
    "plt.rcParams['ytick.labelsize'] = 10\n",
    "plt.rcParams['legend.fontsize'] = 10\n",
    "plt.rcParams['figure.titlesize'] = 15\n",
    "\n",
    "# Configuration\n",
    "validate_or_submit = 'submit'\n",
    "verbose = True\n",
    "create_visualizations = True\n",
    "save_plots = True\n",
    "plot_dir = 'figures'\n",
    "\n",
    "if save_plots:\n",
    "    os.makedirs(plot_dir, exist_ok=True)\n",
    "    print(f\"üìÅ Created visualization directory: {plot_dir}\")\n",
    "\n",
    "# Performance tracking\n",
    "start_time = time.time()\n",
    "performance_metrics = {\n",
    "    'configurations_processed': 0,\n",
    "    'single_mouse_batches': 0,\n",
    "    'pair_batches': 0,\n",
    "    'features_extracted': 0,\n",
    "    'predictions_made': 0,\n",
    "    'models_trained': 0,\n",
    "    'actions_processed': 0\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" \"*15 + \"üê≠ MABe CHALLENGE - ENHANCED VISUALIZATION VERSION üê≠\")\n",
    "print(\"=\"*80)\n",
    "print(f\"üìÖ Start Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"üîß Mode: {validate_or_submit.upper()}\")\n",
    "print(f\"üìä Visualizations: {'ENABLED' if create_visualizations else 'DISABLED'}\")\n",
    "print(f\"üíæ Save Plots: {'YES' if save_plots else 'NO'}\")\n",
    "print(\"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ff82d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8208ee28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lab_id</th>\n",
       "      <th>video_id</th>\n",
       "      <th>mouse1_strain</th>\n",
       "      <th>mouse1_color</th>\n",
       "      <th>mouse1_sex</th>\n",
       "      <th>mouse1_id</th>\n",
       "      <th>mouse1_age</th>\n",
       "      <th>mouse1_condition</th>\n",
       "      <th>mouse2_strain</th>\n",
       "      <th>mouse2_color</th>\n",
       "      <th>...</th>\n",
       "      <th>pix_per_cm_approx</th>\n",
       "      <th>video_width_pix</th>\n",
       "      <th>video_height_pix</th>\n",
       "      <th>arena_width_cm</th>\n",
       "      <th>arena_height_cm</th>\n",
       "      <th>arena_shape</th>\n",
       "      <th>arena_type</th>\n",
       "      <th>body_parts_tracked</th>\n",
       "      <th>behaviors_labeled</th>\n",
       "      <th>tracking_method</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AdaptableSnail</td>\n",
       "      <td>44566106</td>\n",
       "      <td>CD-1 (ICR)</td>\n",
       "      <td>white</td>\n",
       "      <td>male</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8-12 weeks</td>\n",
       "      <td>wireless device</td>\n",
       "      <td>CD-1 (ICR)</td>\n",
       "      <td>white</td>\n",
       "      <td>...</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1228</td>\n",
       "      <td>1068</td>\n",
       "      <td>60.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>square</td>\n",
       "      <td>familiar</td>\n",
       "      <td>[\"body_center\", \"ear_left\", \"ear_right\", \"head...</td>\n",
       "      <td>[\"mouse1,mouse2,approach\", \"mouse1,mouse2,atta...</td>\n",
       "      <td>DeepLabCut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AdaptableSnail</td>\n",
       "      <td>143861384</td>\n",
       "      <td>CD-1 (ICR)</td>\n",
       "      <td>white</td>\n",
       "      <td>male</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8-12 weeks</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CD-1 (ICR)</td>\n",
       "      <td>white</td>\n",
       "      <td>...</td>\n",
       "      <td>9.7</td>\n",
       "      <td>968</td>\n",
       "      <td>608</td>\n",
       "      <td>60.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>square</td>\n",
       "      <td>familiar</td>\n",
       "      <td>[\"body_center\", \"ear_left\", \"ear_right\", \"late...</td>\n",
       "      <td>[\"mouse1,mouse2,approach\", \"mouse1,mouse2,atta...</td>\n",
       "      <td>DeepLabCut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AdaptableSnail</td>\n",
       "      <td>209576908</td>\n",
       "      <td>CD-1 (ICR)</td>\n",
       "      <td>white</td>\n",
       "      <td>male</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8-12 weeks</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CD-1 (ICR)</td>\n",
       "      <td>white</td>\n",
       "      <td>...</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1266</td>\n",
       "      <td>1100</td>\n",
       "      <td>60.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>square</td>\n",
       "      <td>familiar</td>\n",
       "      <td>[\"body_center\", \"ear_left\", \"ear_right\", \"late...</td>\n",
       "      <td>[\"mouse1,mouse2,approach\", \"mouse1,mouse2,atta...</td>\n",
       "      <td>DeepLabCut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AdaptableSnail</td>\n",
       "      <td>278643799</td>\n",
       "      <td>CD-1 (ICR)</td>\n",
       "      <td>white</td>\n",
       "      <td>male</td>\n",
       "      <td>11.0</td>\n",
       "      <td>8-12 weeks</td>\n",
       "      <td>wireless device</td>\n",
       "      <td>CD-1 (ICR)</td>\n",
       "      <td>white</td>\n",
       "      <td>...</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1224</td>\n",
       "      <td>1100</td>\n",
       "      <td>60.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>square</td>\n",
       "      <td>familiar</td>\n",
       "      <td>[\"body_center\", \"ear_left\", \"ear_right\", \"head...</td>\n",
       "      <td>[\"mouse1,mouse2,approach\", \"mouse1,mouse2,atta...</td>\n",
       "      <td>DeepLabCut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AdaptableSnail</td>\n",
       "      <td>351967631</td>\n",
       "      <td>CD-1 (ICR)</td>\n",
       "      <td>white</td>\n",
       "      <td>male</td>\n",
       "      <td>14.0</td>\n",
       "      <td>8-12 weeks</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CD-1 (ICR)</td>\n",
       "      <td>white</td>\n",
       "      <td>...</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1204</td>\n",
       "      <td>1068</td>\n",
       "      <td>60.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>square</td>\n",
       "      <td>familiar</td>\n",
       "      <td>[\"body_center\", \"ear_left\", \"ear_right\", \"late...</td>\n",
       "      <td>[\"mouse1,mouse2,approach\", \"mouse1,mouse2,atta...</td>\n",
       "      <td>DeepLabCut</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           lab_id   video_id mouse1_strain mouse1_color mouse1_sex  mouse1_id  \\\n",
       "0  AdaptableSnail   44566106    CD-1 (ICR)        white       male       10.0   \n",
       "1  AdaptableSnail  143861384    CD-1 (ICR)        white       male        3.0   \n",
       "2  AdaptableSnail  209576908    CD-1 (ICR)        white       male        7.0   \n",
       "3  AdaptableSnail  278643799    CD-1 (ICR)        white       male       11.0   \n",
       "4  AdaptableSnail  351967631    CD-1 (ICR)        white       male       14.0   \n",
       "\n",
       "   mouse1_age mouse1_condition mouse2_strain mouse2_color  ...  \\\n",
       "0  8-12 weeks  wireless device    CD-1 (ICR)        white  ...   \n",
       "1  8-12 weeks              NaN    CD-1 (ICR)        white  ...   \n",
       "2  8-12 weeks              NaN    CD-1 (ICR)        white  ...   \n",
       "3  8-12 weeks  wireless device    CD-1 (ICR)        white  ...   \n",
       "4  8-12 weeks              NaN    CD-1 (ICR)        white  ...   \n",
       "\n",
       "  pix_per_cm_approx  video_width_pix video_height_pix arena_width_cm  \\\n",
       "0              16.0             1228             1068           60.0   \n",
       "1               9.7              968              608           60.0   \n",
       "2              16.0             1266             1100           60.0   \n",
       "3              16.0             1224             1100           60.0   \n",
       "4              16.0             1204             1068           60.0   \n",
       "\n",
       "  arena_height_cm arena_shape arena_type  \\\n",
       "0            60.0      square   familiar   \n",
       "1            60.0      square   familiar   \n",
       "2            60.0      square   familiar   \n",
       "3            60.0      square   familiar   \n",
       "4            60.0      square   familiar   \n",
       "\n",
       "                                  body_parts_tracked  \\\n",
       "0  [\"body_center\", \"ear_left\", \"ear_right\", \"head...   \n",
       "1  [\"body_center\", \"ear_left\", \"ear_right\", \"late...   \n",
       "2  [\"body_center\", \"ear_left\", \"ear_right\", \"late...   \n",
       "3  [\"body_center\", \"ear_left\", \"ear_right\", \"head...   \n",
       "4  [\"body_center\", \"ear_left\", \"ear_right\", \"late...   \n",
       "\n",
       "                                   behaviors_labeled tracking_method  \n",
       "0  [\"mouse1,mouse2,approach\", \"mouse1,mouse2,atta...      DeepLabCut  \n",
       "1  [\"mouse1,mouse2,approach\", \"mouse1,mouse2,atta...      DeepLabCut  \n",
       "2  [\"mouse1,mouse2,approach\", \"mouse1,mouse2,atta...      DeepLabCut  \n",
       "3  [\"mouse1,mouse2,approach\", \"mouse1,mouse2,atta...      DeepLabCut  \n",
       "4  [\"mouse1,mouse2,approach\", \"mouse1,mouse2,atta...      DeepLabCut  \n",
       "\n",
       "[5 rows x 38 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b0a7869",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lab_id</th>\n",
       "      <th>video_id</th>\n",
       "      <th>mouse1_strain</th>\n",
       "      <th>mouse1_color</th>\n",
       "      <th>mouse1_sex</th>\n",
       "      <th>mouse1_id</th>\n",
       "      <th>mouse1_age</th>\n",
       "      <th>mouse1_condition</th>\n",
       "      <th>mouse2_strain</th>\n",
       "      <th>mouse2_color</th>\n",
       "      <th>...</th>\n",
       "      <th>pix_per_cm_approx</th>\n",
       "      <th>video_width_pix</th>\n",
       "      <th>video_height_pix</th>\n",
       "      <th>arena_width_cm</th>\n",
       "      <th>arena_height_cm</th>\n",
       "      <th>arena_shape</th>\n",
       "      <th>arena_type</th>\n",
       "      <th>body_parts_tracked</th>\n",
       "      <th>behaviors_labeled</th>\n",
       "      <th>tracking_method</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AdaptableSnail</td>\n",
       "      <td>438887472</td>\n",
       "      <td>CD-1 (ICR)</td>\n",
       "      <td>white</td>\n",
       "      <td>male</td>\n",
       "      <td>13.0</td>\n",
       "      <td>8-12 weeks</td>\n",
       "      <td>wireless device</td>\n",
       "      <td>CD-1 (ICR)</td>\n",
       "      <td>white</td>\n",
       "      <td>...</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1214</td>\n",
       "      <td>1090</td>\n",
       "      <td>60.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>square</td>\n",
       "      <td>familiar</td>\n",
       "      <td>[\"body_center\", \"ear_left\", \"ear_right\", \"head...</td>\n",
       "      <td>[\"mouse1,mouse2,approach\", \"mouse1,mouse2,atta...</td>\n",
       "      <td>DeepLabCut</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows √ó 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           lab_id   video_id mouse1_strain mouse1_color mouse1_sex  mouse1_id  \\\n",
       "0  AdaptableSnail  438887472    CD-1 (ICR)        white       male       13.0   \n",
       "\n",
       "   mouse1_age mouse1_condition mouse2_strain mouse2_color  ...  \\\n",
       "0  8-12 weeks  wireless device    CD-1 (ICR)        white  ...   \n",
       "\n",
       "  pix_per_cm_approx  video_width_pix video_height_pix arena_width_cm  \\\n",
       "0              16.0             1214             1090           60.0   \n",
       "\n",
       "  arena_height_cm arena_shape arena_type  \\\n",
       "0            60.0      square   familiar   \n",
       "\n",
       "                                  body_parts_tracked  \\\n",
       "0  [\"body_center\", \"ear_left\", \"ear_right\", \"head...   \n",
       "\n",
       "                                   behaviors_labeled tracking_method  \n",
       "0  [\"mouse1,mouse2,approach\", \"mouse1,mouse2,atta...      DeepLabCut  \n",
       "\n",
       "[1 rows x 38 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fee88c7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8790, 38), (1, 38))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a240ffb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agent_id</th>\n",
       "      <th>target_id</th>\n",
       "      <th>action</th>\n",
       "      <th>start_frame</th>\n",
       "      <th>stop_frame</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>rear</td>\n",
       "      <td>4</td>\n",
       "      <td>139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>avoid</td>\n",
       "      <td>13</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>rear</td>\n",
       "      <td>121</td>\n",
       "      <td>172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>rear</td>\n",
       "      <td>156</td>\n",
       "      <td>213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>rear</td>\n",
       "      <td>208</td>\n",
       "      <td>261</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   agent_id  target_id action  start_frame  stop_frame\n",
       "0         2          2   rear            4         139\n",
       "1         4          2  avoid           13          52\n",
       "2         4          4   rear          121         172\n",
       "3         3          3   rear          156         213\n",
       "4         4          4   rear          208         261"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = pd.read_parquet('data/train/annotation/AdaptableSnail/44566106.parquet')\n",
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a44eea8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(   agent_id  target_id action  start_frame  stop_frame\n",
       " 0         2          2   rear            4         139\n",
       " 1         4          2  avoid           13          52\n",
       " 2         4          4   rear          121         172\n",
       " 3         3          3   rear          156         213\n",
       " 4         4          4   rear          208         261,\n",
       " (342, 5))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = pd.read_parquet('data/train/annotation/AdaptableSnail/44566106.parquet')\n",
    "y.head(), y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff95855e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Basic usage - reads all parquet files recursively\\ndf = read_parquet_data_recursive('_provided/test_tracking')\\n\\n# Access metadata\\nprint(df[['source_directory', 'source_subdirectory', 'source_filename']].head())\\n\\n# Filter by specific category\\nadaptable_snail_data = df[df['source_directory'] == 'AdaptableSnail']\\n\\n# Advanced usage with dynamic depth handling\\ndf_advanced = read_parquet_with_custom_metadata('_provided/test_tracking', max_depth=2)\\n\\n# See all unique directories\\nprint(df['source_directory'].unique())\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Union, List, Optional\n",
    "\n",
    "def read_parquet_data_recursive(\n",
    "    base_path: Union[str, Path],\n",
    "    file_pattern: str = \"*.parquet\",\n",
    "    columns: Optional[List[str]] = None,\n",
    "    add_metadata: bool = True\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Recursively read all parquet files from a directory structure.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    base_path : str or Path\n",
    "        Base directory to start recursive search\n",
    "    file_pattern : str, default \"*.parquet\"\n",
    "        Pattern to match files (e.g., \"*.parquet\", \"*.csv\")\n",
    "    columns : list, optional\n",
    "        Specific columns to read from parquet files\n",
    "    add_metadata : bool, default True\n",
    "        Whether to add metadata columns (directory, subdirectory, filename)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Combined dataframe with all parquet data and metadata columns\n",
    "    \"\"\"\n",
    "    base_path = Path(base_path)\n",
    "    \n",
    "    if not base_path.exists():\n",
    "        raise ValueError(f\"Path does not exist: {base_path}\")\n",
    "    \n",
    "    # Recursively find all parquet files\n",
    "    parquet_files = list(base_path.rglob(file_pattern))\n",
    "    \n",
    "    if not parquet_files:\n",
    "        raise ValueError(f\"No files matching '{file_pattern}' found in {base_path}\")\n",
    "    \n",
    "    print(f\"Found {len(parquet_files)} parquet files to process...\")\n",
    "    \n",
    "    # Read and combine all parquet files\n",
    "    dfs = []\n",
    "    for parquet_file in parquet_files:\n",
    "        try:\n",
    "            # Read the parquet file\n",
    "            df = pd.read_parquet(parquet_file, columns=columns)\n",
    "            \n",
    "            if add_metadata:\n",
    "                # Get relative path from base directory\n",
    "                relative_path = parquet_file.relative_to(base_path)\n",
    "                \n",
    "                # Extract directory information\n",
    "                parts = relative_path.parts\n",
    "                \n",
    "                # Add metadata columns\n",
    "                df['source_filename'] = parquet_file.name\n",
    "                df['source_directory'] = parts[0] if len(parts) > 1 else ''\n",
    "                df['source_subdirectory'] = parts[1] if len(parts) > 2 else ''\n",
    "                df['source_full_path'] = str(relative_path)\n",
    "                \n",
    "            dfs.append(df)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {parquet_file}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if not dfs:\n",
    "        raise ValueError(\"No parquet files could be successfully read\")\n",
    "    \n",
    "    # Combine all dataframes\n",
    "    combined_df = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    print(f\"Successfully combined {len(dfs)} files into dataframe with {len(combined_df)} rows\")\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "\n",
    "def read_parquet_with_custom_metadata(\n",
    "    base_path: Union[str, Path],\n",
    "    max_depth: Optional[int] = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Advanced version that handles arbitrary directory depth.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    base_path : str or Path\n",
    "        Base directory to start recursive search\n",
    "    max_depth : int, optional\n",
    "        Maximum directory depth to traverse (None for unlimited)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Combined dataframe with flexible metadata columns\n",
    "    \"\"\"\n",
    "    base_path = Path(base_path)\n",
    "    parquet_files = list(base_path.rglob(\"*.parquet\"))\n",
    "    \n",
    "    dfs = []\n",
    "    for parquet_file in parquet_files:\n",
    "        try:\n",
    "            # Check depth limit\n",
    "            relative_path = parquet_file.relative_to(base_path)\n",
    "            depth = len(relative_path.parts) - 1  # Subtract filename\n",
    "            \n",
    "            if max_depth is not None and depth > max_depth:\n",
    "                continue\n",
    "            \n",
    "            df = pd.read_parquet(parquet_file)\n",
    "            \n",
    "            # Add comprehensive metadata\n",
    "            df['filename'] = parquet_file.name\n",
    "            df['file_stem'] = parquet_file.stem  # Filename without extension\n",
    "            \n",
    "            # Add each directory level as separate column\n",
    "            parts = relative_path.parent.parts\n",
    "            for i, part in enumerate(parts):\n",
    "                df[f'dir_level_{i}'] = part\n",
    "            \n",
    "            # Add full relative path\n",
    "            df['full_relative_path'] = str(relative_path)\n",
    "            df['depth_level'] = depth\n",
    "            \n",
    "            dfs.append(df)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {parquet_file}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()\n",
    "\n",
    "\n",
    "# Example usage for your data structure:\n",
    "\"\"\"\n",
    "# Basic usage - reads all parquet files recursively\n",
    "df = read_parquet_data_recursive('_provided/test_tracking')\n",
    "\n",
    "# Access metadata\n",
    "print(df[['source_directory', 'source_subdirectory', 'source_filename']].head())\n",
    "\n",
    "# Filter by specific category\n",
    "adaptable_snail_data = df[df['source_directory'] == 'AdaptableSnail']\n",
    "\n",
    "# Advanced usage with dynamic depth handling\n",
    "df_advanced = read_parquet_with_custom_metadata('_provided/test_tracking', max_depth=2)\n",
    "\n",
    "# See all unique directories\n",
    "print(df['source_directory'].unique())\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec0d56a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Basic usage - reads all parquet files recursively\n",
    "# df_ta = read_parquet_data_recursive('data/train_annotation')\n",
    "# df_ta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34a983f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_tt = read_parquet_data_recursive('data/train_tracking')\n",
    "# df_tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6ab1ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test_tracking = read_parquet_data_recursive('data/test_tracking')\n",
    "# df_test_tracking.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6152697",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parquet_generator(\n",
    "    base_path: Union[str, Path],\n",
    "    file_pattern: str = \"*.parquet\",\n",
    "    columns: Optional[List[str]] = None,\n",
    "    add_metadata: bool = True,\n",
    "    batch_size: int = 1000\n",
    ") -> Iterator[Dict]:\n",
    "    \"\"\"\n",
    "    Memory-efficient generator that yields rows from parquet files.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    base_path : str or Path\n",
    "        Base directory to start recursive search\n",
    "    file_pattern : str\n",
    "        Pattern to match files\n",
    "    columns : list, optional\n",
    "        Specific columns to read\n",
    "    add_metadata : bool\n",
    "        Whether to add metadata columns\n",
    "    batch_size : int\n",
    "        Number of rows to read at once from each file\n",
    "        \n",
    "    Yields:\n",
    "    -------\n",
    "    dict\n",
    "        Individual row as dictionary\n",
    "    \"\"\"\n",
    "    base_path = Path(base_path)\n",
    "    parquet_files = list(base_path.rglob(file_pattern))\n",
    "    \n",
    "    print(f\"Found {len(parquet_files)} parquet files to process...\")\n",
    "    \n",
    "    for parquet_file in parquet_files:\n",
    "        try:\n",
    "            # Use PyArrow to read in batches for memory efficiency\n",
    "            parquet_reader = pq.ParquetFile(parquet_file)\n",
    "            \n",
    "            # Get relative path info\n",
    "            relative_path = parquet_file.relative_to(base_path)\n",
    "            parts = relative_path.parts\n",
    "            \n",
    "            # Read file in batches\n",
    "            for batch in parquet_reader.iter_batches(batch_size=batch_size, columns=columns):\n",
    "                # Convert batch to pandas for easier manipulation\n",
    "                df_batch = batch.to_pandas()\n",
    "                \n",
    "                if add_metadata:\n",
    "                    df_batch['source_filename'] = parquet_file.name\n",
    "                    df_batch['source_directory'] = parts[0] if len(parts) > 1 else ''\n",
    "                    df_batch['source_subdirectory'] = parts[1] if len(parts) > 2 else ''\n",
    "                    df_batch['source_full_path'] = str(relative_path)\n",
    "                \n",
    "                # Yield each row individually\n",
    "                for _, row in df_batch.iterrows():\n",
    "                    yield row.to_dict()\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {parquet_file}: {e}\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79ed6a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_unified_dataset_dict_efficient(\n",
    "    base_path: Union[str, Path],\n",
    "    columns: Optional[List[str]] = None,\n",
    "    writer_batch_size: int = 1000,\n",
    "    batch_size: int = 1000\n",
    ") -> DatasetDict:\n",
    "    \"\"\"\n",
    "    Memory-efficient version using generators to create DatasetDict.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    base_path : str or Path\n",
    "        Base directory (should contain 'train' and 'test' folders)\n",
    "    columns : list, optional\n",
    "        Specific columns to read from parquet files\n",
    "    writer_batch_size : int\n",
    "        Number of rows to write at once (controls memory usage)\n",
    "    batch_size : int\n",
    "        Number of rows to read at once from parquet files\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    DatasetDict\n",
    "        Single DatasetDict with all splits\n",
    "    \"\"\"\n",
    "    base_path = Path(base_path)\n",
    "    dataset_dict = {}\n",
    "    \n",
    "    structure = {\n",
    "        'train': ['annotation', 'tracking'],\n",
    "        'test': ['tracking']\n",
    "    }\n",
    "    \n",
    "    for split, subsets in structure.items():\n",
    "        for subset in subsets:\n",
    "            split_name = f\"{split}_{subset}\"\n",
    "            subset_path = base_path / split / subset\n",
    "            \n",
    "            if not subset_path.exists():\n",
    "                print(f\"Warning: Path does not exist: {subset_path}\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"\\nProcessing {split_name} with generator...\")\n",
    "            \n",
    "            # Create generator for this split/subset\n",
    "            def gen():\n",
    "                for row in parquet_generator(\n",
    "                    subset_path,\n",
    "                    columns=columns,\n",
    "                    add_metadata=True,\n",
    "                    batch_size=batch_size\n",
    "                ):\n",
    "                    row['split'] = split\n",
    "                    row['subset'] = subset\n",
    "                    yield row\n",
    "            \n",
    "            # Use from_generator for memory-efficient loading\n",
    "            dataset_dict[split_name] = Dataset.from_generator(\n",
    "                gen,\n",
    "                writer_batch_size=writer_batch_size\n",
    "            )\n",
    "    \n",
    "    return DatasetDict(dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9efcc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def upload_to_huggingface(\n",
    "    dataset_dict: DatasetDict,\n",
    "    repo_name: str,\n",
    "    private: bool = False,\n",
    "    token: Optional[str] = None,\n",
    "    max_shard_size: str = \"500MB\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Upload DatasetDict to Hugging Face Hub with memory-efficient settings.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    dataset_dict : DatasetDict\n",
    "        The dataset dictionary to upload\n",
    "    repo_name : str\n",
    "        Repository name in format \"username/dataset-name\"\n",
    "    private : bool\n",
    "        Whether to make the dataset private\n",
    "    token : str, optional\n",
    "        Hugging Face authentication token\n",
    "    max_shard_size : str\n",
    "        Maximum size of each shard file (e.g., \"500MB\", \"1GB\")\n",
    "    \"\"\"\n",
    "    print(f\"\\nUploading dataset to {repo_name}...\")\n",
    "    print(f\"Dataset splits: {list(dataset_dict.keys())}\")\n",
    "    \n",
    "    for split_name, dataset in dataset_dict.items():\n",
    "        print(f\"  - {split_name}: {len(dataset)} rows\")\n",
    "    \n",
    "    # Push to hub with sharding for large datasets\n",
    "    dataset_dict.push_to_hub(\n",
    "        repo_name,\n",
    "        private=private,\n",
    "        token=token,\n",
    "        max_shard_size=max_shard_size\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n‚úì Successfully uploaded to: https://huggingface.co/datasets/{repo_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5682faff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_from_pyarrow_tables(\n",
    "    base_path: Union[str, Path],\n",
    "    columns: Optional[List[str]] = None\n",
    ") -> DatasetDict:\n",
    "    \"\"\"\n",
    "    Most memory-efficient version using PyArrow tables directly.\n",
    "    Avoids pandas entirely and works with Arrow format throughout.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    base_path : str or Path\n",
    "        Base directory\n",
    "    columns : list, optional\n",
    "        Specific columns to read\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    DatasetDict\n",
    "        Single DatasetDict with all splits\n",
    "    \"\"\"\n",
    "    base_path = Path(base_path)\n",
    "    dataset_dict = {}\n",
    "    \n",
    "    structure = {\n",
    "        'train': ['annotation', 'tracking'],\n",
    "        'test': ['tracking']\n",
    "    }\n",
    "    \n",
    "    for split, subsets in structure.items():\n",
    "        for subset in subsets:\n",
    "            split_name = f\"{split}_{subset}\"\n",
    "            subset_path = base_path / split / subset\n",
    "            \n",
    "            if not subset_path.exists():\n",
    "                print(f\"Warning: Path does not exist: {subset_path}\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"\\nProcessing {split_name} with PyArrow...\")\n",
    "            \n",
    "            # Find all parquet files\n",
    "            parquet_files = list(subset_path.rglob(\"*.parquet\"))\n",
    "            tables = []\n",
    "            \n",
    "            for parquet_file in parquet_files:\n",
    "                try:\n",
    "                    # Read parquet file as PyArrow table\n",
    "                    table = pq.read_table(parquet_file, columns=columns)\n",
    "                    \n",
    "                    # Add metadata columns\n",
    "                    relative_path = parquet_file.relative_to(subset_path)\n",
    "                    parts = relative_path.parts\n",
    "                    \n",
    "                    # Add metadata as new columns to the table\n",
    "                    table = table.append_column(\n",
    "                        'source_filename',\n",
    "                        pa.array([parquet_file.name] * len(table))\n",
    "                    )\n",
    "                    table = table.append_column(\n",
    "                        'source_directory',\n",
    "                        pa.array([parts[0] if len(parts) > 1 else ''] * len(table))\n",
    "                    )\n",
    "                    table = table.append_column(\n",
    "                        'source_subdirectory',\n",
    "                        pa.array([parts[1] if len(parts) > 2 else ''] * len(table))\n",
    "                    )\n",
    "                    table = table.append_column(\n",
    "                        'source_full_path',\n",
    "                        pa.array([str(relative_path)] * len(table))\n",
    "                    )\n",
    "                    table = table.append_column(\n",
    "                        'split',\n",
    "                        pa.array([split] * len(table))\n",
    "                    )\n",
    "                    table = table.append_column(\n",
    "                        'subset',\n",
    "                        pa.array([subset] * len(table))\n",
    "                    )\n",
    "                    \n",
    "                    tables.append(table)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading {parquet_file}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            if tables:\n",
    "                # Concatenate all tables\n",
    "                combined_table = pa.concat_tables(tables)\n",
    "                # Create dataset directly from PyArrow table\n",
    "                dataset_dict[split_name] = Dataset(combined_table)\n",
    "            \n",
    "    return DatasetDict(dataset_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "602cd799",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Union, List, Optional, Dict, Iterator, Tuple\n",
    "from datasets import Dataset, DatasetDict\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def scan_schemas(\n",
    "    base_path: Union[str, Path],\n",
    "    file_pattern: str = \"*.parquet\"\n",
    ") -> Dict[str, List[Tuple[Path, pa.Schema]]]:\n",
    "    \"\"\"\n",
    "    Scan all parquet files and collect their schemas for validation.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    base_path : str or Path\n",
    "        Base directory to scan\n",
    "    file_pattern : str\n",
    "        Pattern to match files\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    Dict[str, List[Tuple[Path, pa.Schema]]]\n",
    "        Dictionary mapping split names to list of (file_path, schema) tuples\n",
    "    \"\"\"\n",
    "    base_path = Path(base_path)\n",
    "    schema_map = defaultdict(list)\n",
    "    \n",
    "    structure = {\n",
    "        'train': ['annotation', 'tracking'],\n",
    "        'test': ['tracking']\n",
    "    }\n",
    "    \n",
    "    for split, subsets in structure.items():\n",
    "        for subset in subsets:\n",
    "            split_name = f\"{split}_{subset}\"\n",
    "            subset_path = base_path / split / subset\n",
    "            \n",
    "            if not subset_path.exists():\n",
    "                continue\n",
    "            \n",
    "            parquet_files = list(subset_path.rglob(file_pattern))\n",
    "            print(f\"Scanning {len(parquet_files)} files in {split_name}...\")\n",
    "            \n",
    "            for parquet_file in parquet_files:\n",
    "                try:\n",
    "                    parquet_meta = pq.read_metadata(parquet_file)\n",
    "                    schema = parquet_meta.schema.to_arrow_schema()\n",
    "                    schema_map[split_name].append((parquet_file, schema))\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading schema from {parquet_file}: {e}\")\n",
    "    \n",
    "    return dict(schema_map)\n",
    "\n",
    "\n",
    "def validate_and_unify_schemas(\n",
    "    schema_map: Dict[str, List[Tuple[Path, pa.Schema]]],\n",
    "    promote_options: str = \"permissive\"\n",
    ") -> Dict[str, pa.Schema]:\n",
    "    \"\"\"\n",
    "    Validate schemas and create unified schemas for each split.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    schema_map : Dict[str, List[Tuple[Path, pa.Schema]]]\n",
    "        Output from scan_schemas()\n",
    "    promote_options : str\n",
    "        Either 'default' or 'permissive'\n",
    "        - 'default': Only null can be unified with another type\n",
    "        - 'permissive': Types are promoted to the greater common denominator\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    Dict[str, pa.Schema]\n",
    "        Unified schema for each split\n",
    "    \"\"\"\n",
    "    unified_schemas = {}\n",
    "    \n",
    "    for split_name, file_schemas in schema_map.items():\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Validating schemas for {split_name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        if not file_schemas:\n",
    "            continue\n",
    "        \n",
    "        # Extract just the schemas\n",
    "        schemas = [schema for _, schema in file_schemas]\n",
    "        \n",
    "        # Check for schema inconsistencies\n",
    "        print(f\"Found {len(schemas)} files\")\n",
    "        \n",
    "        # Group files by schema signature\n",
    "        schema_groups = defaultdict(list)\n",
    "        for file_path, schema in file_schemas:\n",
    "            # Create a signature based on field names and types\n",
    "            signature = tuple((field.name, str(field.type)) for field in schema)\n",
    "            schema_groups[signature].append(file_path)\n",
    "        \n",
    "        if len(schema_groups) > 1:\n",
    "            print(f\"\\n‚ö†Ô∏è  WARNING: Found {len(schema_groups)} different schemas!\")\n",
    "            for i, (signature, files) in enumerate(schema_groups.items(), 1):\n",
    "                print(f\"\\nSchema variant {i}: ({len(files)} files)\")\n",
    "                # Show just the schema fields\n",
    "                for field_name, field_type in signature:\n",
    "                    print(f\"  - {field_name}: {field_type}\")\n",
    "                print(f\"  Example file: {files[0]}\")\n",
    "        else:\n",
    "            print(\"‚úì All schemas are identical\")\n",
    "        \n",
    "        # Attempt to unify schemas\n",
    "        try:\n",
    "            unified_schema = pa.unify_schemas(schemas, promote_options=promote_options)\n",
    "            unified_schemas[split_name] = unified_schema\n",
    "            \n",
    "            print(f\"\\n‚úì Successfully unified schema:\")\n",
    "            for field in unified_schema:\n",
    "                print(f\"  - {field.name}: {field.type}\")\n",
    "                \n",
    "        except pa.ArrowInvalid as e:\n",
    "            print(f\"\\n‚ùå ERROR: Cannot unify schemas for {split_name}\")\n",
    "            print(f\"Error: {e}\")\n",
    "            print(\"\\nTrying with 'permissive' mode...\")\n",
    "            \n",
    "            try:\n",
    "                unified_schema = pa.unify_schemas(schemas, promote_options=\"permissive\")\n",
    "                unified_schemas[split_name] = unified_schema\n",
    "                print(f\"‚úì Successfully unified with permissive mode\")\n",
    "                for field in unified_schema:\n",
    "                    print(f\"  - {field.name}: {field.type}\")\n",
    "            except pa.ArrowInvalid as e2:\n",
    "                print(f\"‚ùå Still failed: {e2}\")\n",
    "                raise\n",
    "    \n",
    "    return unified_schemas\n",
    "\n",
    "\n",
    "def create_dataset_from_pyarrow_tables_with_schema(\n",
    "    base_path: Union[str, Path],\n",
    "    unified_schemas: Dict[str, pa.Schema],\n",
    "    columns: Optional[List[str]] = None\n",
    ") -> DatasetDict:\n",
    "    \"\"\"\n",
    "    Memory-efficient version using PyArrow tables with unified schema enforcement.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    base_path : str or Path\n",
    "        Base directory\n",
    "    unified_schemas : Dict[str, pa.Schema]\n",
    "        Unified schemas from validate_and_unify_schemas()\n",
    "    columns : list, optional\n",
    "        Specific columns to read\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    DatasetDict\n",
    "        Single DatasetDict with all splits\n",
    "    \"\"\"\n",
    "    base_path = Path(base_path)\n",
    "    dataset_dict = {}\n",
    "    \n",
    "    structure = {\n",
    "        'train': ['annotation', 'tracking'],\n",
    "        'test': ['tracking']\n",
    "    }\n",
    "    \n",
    "    for split, subsets in structure.items():\n",
    "        for subset in subsets:\n",
    "            split_name = f\"{split}_{subset}\"\n",
    "            subset_path = base_path / split / subset\n",
    "            \n",
    "            if not subset_path.exists():\n",
    "                print(f\"Warning: Path does not exist: {subset_path}\")\n",
    "                continue\n",
    "            \n",
    "            if split_name not in unified_schemas:\n",
    "                print(f\"Warning: No unified schema for {split_name}\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"\\nProcessing {split_name} with unified schema...\")\n",
    "            \n",
    "            # Get the base unified schema (without metadata columns)\n",
    "            base_schema = unified_schemas[split_name]\n",
    "            \n",
    "            # Find all parquet files\n",
    "            parquet_files = list(subset_path.rglob(\"*.parquet\"))\n",
    "            tables = []\n",
    "            \n",
    "            for parquet_file in parquet_files:\n",
    "                try:\n",
    "                    # Read parquet file as PyArrow table\n",
    "                    table = pq.read_table(parquet_file, columns=columns)\n",
    "                    \n",
    "                    # Cast to unified schema to ensure consistency\n",
    "                    # Only cast columns that are in the base schema\n",
    "                    cols_to_cast = [field.name for field in base_schema if field.name in table.column_names]\n",
    "                    if cols_to_cast:\n",
    "                        cast_schema = pa.schema([\n",
    "                            base_schema.field(name) for name in cols_to_cast\n",
    "                        ])\n",
    "                        # Cast only the existing columns\n",
    "                        table = table.cast(cast_schema)\n",
    "                    \n",
    "                    # Add metadata columns\n",
    "                    relative_path = parquet_file.relative_to(subset_path)\n",
    "                    parts = relative_path.parts\n",
    "                    \n",
    "                    table = table.append_column(\n",
    "                        'source_filename',\n",
    "                        pa.array([parquet_file.name] * len(table))\n",
    "                    )\n",
    "                    table = table.append_column(\n",
    "                        'source_directory',\n",
    "                        pa.array([parts[0] if len(parts) > 1 else ''] * len(table))\n",
    "                    )\n",
    "                    table = table.append_column(\n",
    "                        'source_subdirectory',\n",
    "                        pa.array([parts[1] if len(parts) > 2 else ''] * len(table))\n",
    "                    )\n",
    "                    table = table.append_column(\n",
    "                        'source_full_path',\n",
    "                        pa.array([str(relative_path)] * len(table))\n",
    "                    )\n",
    "                    table = table.append_column(\n",
    "                        'split',\n",
    "                        pa.array([split] * len(table))\n",
    "                    )\n",
    "                    table = table.append_column(\n",
    "                        'subset',\n",
    "                        pa.array([subset] * len(table))\n",
    "                    )\n",
    "                    \n",
    "                    tables.append(table)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading {parquet_file}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            if tables:\n",
    "                # Concatenate all tables\n",
    "                combined_table = pa.concat_tables(tables)\n",
    "                # Create dataset directly from PyArrow table\n",
    "                dataset_dict[split_name] = Dataset(combined_table)\n",
    "            \n",
    "    return DatasetDict(dataset_dict)\n",
    "\n",
    "\n",
    "def validate_schemas_and_create_dataset(\n",
    "    base_path: Union[str, Path],\n",
    "    columns: Optional[List[str]] = None,\n",
    "    promote_options: str = \"permissive\"\n",
    ") -> DatasetDict:\n",
    "    \"\"\"\n",
    "    Complete workflow: scan schemas, validate, unify, and create dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    base_path : str or Path\n",
    "        Base directory\n",
    "    columns : list, optional\n",
    "        Specific columns to read\n",
    "    promote_options : str\n",
    "        'default' or 'permissive' for schema unification\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    DatasetDict\n",
    "        Complete dataset dictionary\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"STEP 1: Scanning schemas\")\n",
    "    print(\"=\"*60)\n",
    "    schema_map = scan_schemas(base_path)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STEP 2: Validating and unifying schemas\")\n",
    "    print(\"=\"*60)\n",
    "    unified_schemas = validate_and_unify_schemas(schema_map, promote_options=promote_options)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STEP 3: Creating dataset with unified schemas\")\n",
    "    print(\"=\"*60)\n",
    "    dataset_dict = create_dataset_from_pyarrow_tables_with_schema(\n",
    "        base_path,\n",
    "        unified_schemas,\n",
    "        columns=columns\n",
    "    )\n",
    "    \n",
    "    return dataset_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c979b78c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "OPTION 1: Generator-based approach (most memory efficient)\n",
      "============================================================\n",
      "\n",
      "Processing train_annotation with generator...\n",
      "\n",
      "Processing train_tracking with generator...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 1548 examples [00:00, 15399.86 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8790 parquet files to process...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 288187614 examples [3:06:03, 25814.04 examples/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mOPTION 1: Generator-based approach (most memory efficient)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m60\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m dataset_dict = \u001b[43mcreate_unified_dataset_dict_efficient\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbase_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwriter_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Write to disk every 500 rows\u001b[39;49;00m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m500\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Read 500 rows at a time from each parquet file\u001b[39;49;00m\n\u001b[32m     18\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# OPTION 2: PyArrow tables (FASTER, moderately memory efficient)\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Best for large datasets (10-100GB) where speed matters\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# Avoids pandas overhead but loads more data into memory\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     30\u001b[39m \n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# Inspect the dataset\u001b[39;00m\n\u001b[32m     32\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m60\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 58\u001b[39m, in \u001b[36mcreate_unified_dataset_dict_efficient\u001b[39m\u001b[34m(base_path, columns, writer_batch_size, batch_size)\u001b[39m\n\u001b[32m     55\u001b[39m                 \u001b[38;5;28;01myield\u001b[39;00m row\n\u001b[32m     57\u001b[39m         \u001b[38;5;66;03m# Use from_generator for memory-efficient loading\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m         dataset_dict[split_name] = \u001b[43mDataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_generator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m            \u001b[49m\u001b[43mwriter_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwriter_batch_size\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m DatasetDict(dataset_dict)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspaces/MABe-2025/.venv/lib/python3.13/site-packages/datasets/arrow_dataset.py:1187\u001b[39m, in \u001b[36mDataset.from_generator\u001b[39m\u001b[34m(generator, features, cache_dir, keep_in_memory, gen_kwargs, num_proc, split, **kwargs)\u001b[39m\n\u001b[32m   1125\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Create a Dataset from a generator.\u001b[39;00m\n\u001b[32m   1126\u001b[39m \n\u001b[32m   1127\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1174\u001b[39m \u001b[33;03m```\u001b[39;00m\n\u001b[32m   1175\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1176\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgenerator\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GeneratorDatasetInputStream\n\u001b[32m   1178\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mGeneratorDatasetInputStream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1179\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1180\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1181\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeep_in_memory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_in_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgen_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgen_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1184\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1185\u001b[39m \u001b[43m    \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m=\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1186\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m1187\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspaces/MABe-2025/.venv/lib/python3.13/site-packages/datasets/io/generator.py:49\u001b[39m, in \u001b[36mGeneratorDatasetInputStream.read\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     46\u001b[39m     verification_mode = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     47\u001b[39m     base_path = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbuilder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdownload_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbase_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     56\u001b[39m     dataset = \u001b[38;5;28mself\u001b[39m.builder.as_dataset(\n\u001b[32m     57\u001b[39m         split=\u001b[38;5;28mself\u001b[39m.builder.config.split, verification_mode=verification_mode, in_memory=\u001b[38;5;28mself\u001b[39m.keep_in_memory\n\u001b[32m     58\u001b[39m     )\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m dataset\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspaces/MABe-2025/.venv/lib/python3.13/site-packages/datasets/builder.py:894\u001b[39m, in \u001b[36mDatasetBuilder.download_and_prepare\u001b[39m\u001b[34m(self, output_dir, download_config, download_mode, verification_mode, dl_manager, base_path, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[39m\n\u001b[32m    892\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m num_proc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    893\u001b[39m     prepare_split_kwargs[\u001b[33m\"\u001b[39m\u001b[33mnum_proc\u001b[39m\u001b[33m\"\u001b[39m] = num_proc\n\u001b[32m--> \u001b[39m\u001b[32m894\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_download_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    895\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    896\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    897\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mprepare_split_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    898\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdownload_and_prepare_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    899\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    900\u001b[39m \u001b[38;5;66;03m# Sync info\u001b[39;00m\n\u001b[32m    901\u001b[39m \u001b[38;5;28mself\u001b[39m.info.dataset_size = \u001b[38;5;28msum\u001b[39m(split.num_bytes \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.info.splits.values())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspaces/MABe-2025/.venv/lib/python3.13/site-packages/datasets/builder.py:1609\u001b[39m, in \u001b[36mGeneratorBasedBuilder._download_and_prepare\u001b[39m\u001b[34m(self, dl_manager, verification_mode, **prepare_splits_kwargs)\u001b[39m\n\u001b[32m   1608\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_download_and_prepare\u001b[39m(\u001b[38;5;28mself\u001b[39m, dl_manager, verification_mode, **prepare_splits_kwargs):\n\u001b[32m-> \u001b[39m\u001b[32m1609\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_download_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1610\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1611\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1612\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheck_duplicate_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[43mVerificationMode\u001b[49m\u001b[43m.\u001b[49m\u001b[43mBASIC_CHECKS\u001b[49m\n\u001b[32m   1613\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[43mVerificationMode\u001b[49m\u001b[43m.\u001b[49m\u001b[43mALL_CHECKS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1614\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mprepare_splits_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1615\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspaces/MABe-2025/.venv/lib/python3.13/site-packages/datasets/builder.py:970\u001b[39m, in \u001b[36mDatasetBuilder._download_and_prepare\u001b[39m\u001b[34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001b[39m\n\u001b[32m    966\u001b[39m split_dict.add(split_generator.split_info)\n\u001b[32m    968\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    969\u001b[39m     \u001b[38;5;66;03m# Prepare split will record examples associated to the split\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m970\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prepare_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mprepare_split_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    972\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[32m    973\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCannot find data file. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    974\u001b[39m         + (\u001b[38;5;28mself\u001b[39m.manual_download_instructions \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    975\u001b[39m         + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mOriginal error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    976\u001b[39m         + \u001b[38;5;28mstr\u001b[39m(e)\n\u001b[32m    977\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspaces/MABe-2025/.venv/lib/python3.13/site-packages/datasets/builder.py:1447\u001b[39m, in \u001b[36mGeneratorBasedBuilder._prepare_split\u001b[39m\u001b[34m(self, split_generator, check_duplicate_keys, file_format, num_proc, max_shard_size)\u001b[39m\n\u001b[32m   1445\u001b[39m job_id = \u001b[32m0\u001b[39m\n\u001b[32m   1446\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m pbar:\n\u001b[32m-> \u001b[39m\u001b[32m1447\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mjob_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prepare_split_single\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1448\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgen_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgen_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjob_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjob_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m_prepare_split_args\u001b[49m\n\u001b[32m   1449\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1450\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1451\u001b[39m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspaces/MABe-2025/.venv/lib/python3.13/site-packages/datasets/builder.py:1568\u001b[39m, in \u001b[36mGeneratorBasedBuilder._prepare_split_single\u001b[39m\u001b[34m(self, gen_kwargs, fpath, file_format, max_shard_size, split_info, check_duplicate_keys, job_id)\u001b[39m\n\u001b[32m   1566\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1567\u001b[39m     _time = time.time()\n\u001b[32m-> \u001b[39m\u001b[32m1568\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1569\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmax_shard_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_num_bytes\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_shard_size\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1570\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnum_examples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_bytes\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfinalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspaces/MABe-2025/.venv/lib/python3.13/site-packages/datasets/packaged_modules/generator/generator.py:33\u001b[39m, in \u001b[36mGenerator._generate_examples\u001b[39m\u001b[34m(self, **gen_kwargs)\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_generate_examples\u001b[39m(\u001b[38;5;28mself\u001b[39m, **gen_kwargs):\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m.config.generator(**gen_kwargs))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 47\u001b[39m, in \u001b[36mcreate_unified_dataset_dict_efficient.<locals>.gen\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgen\u001b[39m():\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mparquet_generator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m        \u001b[49m\u001b[43msubset_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m        \u001b[49m\u001b[43madd_metadata\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msplit\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msubset\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43msubset\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 55\u001b[39m, in \u001b[36mparquet_generator\u001b[39m\u001b[34m(base_path, file_pattern, columns, add_metadata, batch_size)\u001b[39m\n\u001b[32m     52\u001b[39m             df_batch[\u001b[33m'\u001b[39m\u001b[33msource_full_path\u001b[39m\u001b[33m'\u001b[39m] = \u001b[38;5;28mstr\u001b[39m(relative_path)\n\u001b[32m     54\u001b[39m         \u001b[38;5;66;03m# Yield each row individually\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdf_batch\u001b[49m\u001b[43m.\u001b[49m\u001b[43miterrows\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspaces/MABe-2025/.venv/lib/python3.13/site-packages/pandas/core/frame.py:1559\u001b[39m, in \u001b[36mDataFrame.iterrows\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1557\u001b[39m using_cow = using_copy_on_write()\n\u001b[32m   1558\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m.index, \u001b[38;5;28mself\u001b[39m.values):\n\u001b[32m-> \u001b[39m\u001b[32m1559\u001b[39m     s = \u001b[43mklass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m.__finalize__(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m   1560\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m using_cow \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._mgr.is_single_block:\n\u001b[32m   1561\u001b[39m         s._mgr.add_references(\u001b[38;5;28mself\u001b[39m._mgr)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspaces/MABe-2025/.venv/lib/python3.13/site-packages/pandas/core/series.py:584\u001b[39m, in \u001b[36mSeries.__init__\u001b[39m\u001b[34m(self, data, index, dtype, name, copy, fastpath)\u001b[39m\n\u001b[32m    582\u001b[39m         data = data.copy()\n\u001b[32m    583\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m584\u001b[39m     data = \u001b[43msanitize_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    586\u001b[39m     manager = _get_option(\u001b[33m\"\u001b[39m\u001b[33mmode.data_manager\u001b[39m\u001b[33m\"\u001b[39m, silent=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    587\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m manager == \u001b[33m\"\u001b[39m\u001b[33mblock\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspaces/MABe-2025/.venv/lib/python3.13/site-packages/pandas/core/construction.py:604\u001b[39m, in \u001b[36msanitize_array\u001b[39m\u001b[34m(data, index, dtype, copy, allow_2d)\u001b[39m\n\u001b[32m    602\u001b[39m subarr = data\n\u001b[32m    603\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m data.dtype == \u001b[38;5;28mobject\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m604\u001b[39m     subarr = \u001b[43mmaybe_infer_to_datetimelike\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    605\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m object_index \u001b[38;5;129;01mand\u001b[39;00m using_string_dtype() \u001b[38;5;129;01mand\u001b[39;00m is_string_dtype(subarr):\n\u001b[32m    606\u001b[39m         \u001b[38;5;66;03m# Avoid inference when string option is set\u001b[39;00m\n\u001b[32m    607\u001b[39m         subarr = data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspaces/MABe-2025/.venv/lib/python3.13/site-packages/pandas/core/dtypes/cast.py:1198\u001b[39m, in \u001b[36mmaybe_infer_to_datetimelike\u001b[39m\u001b[34m(value, convert_to_nullable_dtype)\u001b[39m\n\u001b[32m   1193\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m value\n\u001b[32m   1195\u001b[39m \u001b[38;5;66;03m# error: Incompatible return value type (got \"Union[ExtensionArray,\u001b[39;00m\n\u001b[32m   1196\u001b[39m \u001b[38;5;66;03m# ndarray[Any, Any]]\", expected \"Union[ndarray[Any, Any], DatetimeArray,\u001b[39;00m\n\u001b[32m   1197\u001b[39m \u001b[38;5;66;03m# TimedeltaArray, PeriodArray, IntervalArray]\")\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1198\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmaybe_convert_objects\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[return-value]\u001b[39;49;00m\n\u001b[32m   1199\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1200\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Here we do not convert numeric dtypes, as if we wanted that,\u001b[39;49;00m\n\u001b[32m   1201\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#  numpy would have done it for us.\u001b[39;49;00m\n\u001b[32m   1202\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconvert_numeric\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1203\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconvert_non_numeric\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1204\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconvert_to_nullable_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_to_nullable_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1205\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype_if_all_nat\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mM8[ns]\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1206\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/lib.pyx:2560\u001b[39m, in \u001b[36mpandas._libs.lib.maybe_convert_objects\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspaces/MABe-2025/.venv/lib/python3.13/site-packages/numpy/_core/numeric.py:387\u001b[39m, in \u001b[36mfull\u001b[39m\u001b[34m(shape, fill_value, dtype, order, device, like)\u001b[39m\n\u001b[32m    385\u001b[39m     dtype = fill_value.dtype\n\u001b[32m    386\u001b[39m a = empty(shape, dtype, order, device=device)\n\u001b[32m--> \u001b[39m\u001b[32m387\u001b[39m \u001b[43mmultiarray\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopyto\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcasting\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43munsafe\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    388\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m a\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspaces/MABe-2025/.venv/lib/python3.13/site-packages/numpy/_core/multiarray.py:1106\u001b[39m, in \u001b[36mcopyto\u001b[39m\u001b[34m(dst, src, casting, where)\u001b[39m\n\u001b[32m   1063\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1064\u001b[39m \u001b[33;03m    unravel_index(indices, shape, order='C')\u001b[39;00m\n\u001b[32m   1065\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1101\u001b[39m \n\u001b[32m   1102\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   1103\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (indices,)\n\u001b[32m-> \u001b[39m\u001b[32m1106\u001b[39m \u001b[38;5;129m@array_function_from_c_func_and_dispatcher\u001b[39m(_multiarray_umath.copyto)\n\u001b[32m   1107\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcopyto\u001b[39m(dst, src, casting=\u001b[38;5;28;01mNone\u001b[39;00m, where=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m   1108\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1109\u001b[39m \u001b[33;03m    copyto(dst, src, casting='same_kind', where=True)\u001b[39;00m\n\u001b[32m   1110\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1151\u001b[39m \n\u001b[32m   1152\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   1153\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (dst, src, where)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# MAIN USAGE - CHOOSE ONE APPROACH\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # OPTION 1: Generator-based (MOST MEMORY EFFICIENT)\n",
    "    # Best for extremely large datasets (100GB+)\n",
    "    # Processes data in small batches and writes to disk incrementally\n",
    "    print(\"=\"*60)\n",
    "    print(\"OPTION 1: Generator-based approach (most memory efficient)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    dataset_dict = create_unified_dataset_dict_efficient(\n",
    "        base_path=\"data\",\n",
    "        writer_batch_size=500,  # Write to disk every 500 rows\n",
    "        batch_size=500  # Read 500 rows at a time from each parquet file\n",
    "    )\n",
    "    \n",
    "    # OPTION 2: PyArrow tables (FASTER, moderately memory efficient)\n",
    "    # Best for large datasets (10-100GB) where speed matters\n",
    "    # Avoids pandas overhead but loads more data into memory\n",
    "    # print(\"=\"*60)\n",
    "    # print(\"OPTION 2: PyArrow table approach (fast and efficient)\")\n",
    "    # print(\"=\"*60)\n",
    "    # \n",
    "    # dataset_dict = create_dataset_from_pyarrow_tables(\n",
    "    #     base_path=\"data\"\n",
    "    # )\n",
    "    \n",
    "    # Inspect the dataset\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Dataset Structure:\")\n",
    "    print(\"=\"*60)\n",
    "    print(dataset_dict)\n",
    "    \n",
    "    # Preview each split\n",
    "    for split_name in dataset_dict.keys():\n",
    "        print(f\"\\n{split_name}:\")\n",
    "        print(f\"  Columns: {dataset_dict[split_name].column_names}\")\n",
    "        print(f\"  Rows: {len(dataset_dict[split_name])}\")\n",
    "        print(f\"  First row: {dataset_dict[split_name][0]}\")\n",
    "    \n",
    "    # Upload to Hugging Face with memory-efficient sharding\n",
    "    upload_to_huggingface(\n",
    "        dataset_dict=dataset_dict,\n",
    "        repo_name=\"mxngjxa/MABe-2025\",\n",
    "        private=True,\n",
    "        max_shard_size=\"500MB\"  # Split large files into 500MB chunks\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39737047",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
